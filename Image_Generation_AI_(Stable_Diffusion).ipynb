{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "authorship_tag": "ABX9TyPupTJTGs86+tn7tCh0HCac",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yf591/Using-Stable-Diffusion-in-Google-Colab/blob/main/Image_Generation_AI_(Stable_Diffusion).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#<font color=fbb03b>事前準備\n",
        "- ライブラリのインストール\n",
        "- ライブラリインポート\n",
        "- Hugging Faceにログイン"
      ],
      "metadata": {
        "id": "uvexwfkWvyIF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Google Drive をマウント\n",
        "from google.colab import drive, output\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 以下はサンプルです.ご自身の環境に合わせてフォルダへのパスは変更してください.\n",
        "%cd /content/drive/My Drive/Colab Notebooks/【画像生成】Stable Dffusion"
      ],
      "metadata": {
        "id": "5y33XP_A2Xh-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9d89fbc-5e0f-4814-a0f5-cf8c48a023c8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/My Drive/Colab Notebooks/【画像生成】Stable Dffusion\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.makedirs(\"/content/drive/My Drive/Colab Notebooks/【画像生成】Stable Dffusion/Image_storage_folder\", exist_ok=True)"
      ],
      "metadata": {
        "id": "zY9qEbgaXrR7"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "collapsed": true,
        "id": "Qr9-paw_rk3k"
      },
      "outputs": [],
      "source": [
        "# 必要なライブラリをインストール\n",
        "!pip install diffusers transformers peft accelerate bitsandbytes\n",
        "output.clear()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**解説**: diffusers, transformers, peft の3つのライブラリをインストールします。これにより、DiffusionPipeline（拡散モデル）や他の機械学習モデルを使用できるようになります。\n",
        "\n",
        "- diffusers: 拡散モデルを実装したライブラリ。\n",
        "- transformers: トランスフォーマーモデル（BERTやGPTなど）を扱うためのライブラリ。\n",
        "- peft: パラメータ効率の良いファインチューニング（PEFT）を行うためのライブラリ。"
      ],
      "metadata": {
        "id": "FNVz3vxGvKGv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 必要なライブラリをインポート\n",
        "import torch  # PyTorchのライブラリをインポート\n",
        "from peft import PeftModel, LoraConfig, get_peft_model\n",
        "\n",
        "# 必要に応じてDiffusionPipelineまたはStableDiffusionPipelineをインポート\n",
        "from diffusers import DiffusionPipeline\n",
        "from diffusers import StableDiffusionPipeline\n",
        "\n",
        "import matplotlib.pyplot as plt # 可視化のためのライブラリをインポート"
      ],
      "metadata": {
        "id": "hCCpuCfRrtU3"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**解説**: torchはPyTorchのコアライブラリをインポートし、DiffusionPipelineは拡散モデルを簡単に利用できるようにするためのクラスを提供しています。\n",
        "\n",
        "**torch**: ディープラーニングのフレームワーク。\n",
        "\n",
        "**DiffusionPipeline**: テキストや画像生成のための拡散モデルパイプラインを提供するクラス。\n",
        "\n",
        "- 説明: DiffusionPipelineは、一般的な拡散モデルを扱うためのベースクラスです。このクラスは、さまざまな拡散モデル（Stable Diffusionを含む）に対して使用できますが、特定のモデル向けの最適化はされていない場合があります。\n",
        "- 主な機能:\n",
        "  - さまざまな拡散モデルを利用するための汎用的な機能を提供します。\n",
        "  - モデルを選択する柔軟性があるため、異なる拡散技術を使いたい場合に便利です。\n",
        "\n",
        "**StableDiffusionPipeline**: 高品質な画像生成のためのパイプラインを提供するクラス。テキストプロンプトを基にリアルな画像を生成する機能を持つ。\n",
        "- 説明: StableDiffusionPipelineは、Stable Diffusionモデル専用のパイプラインクラスです。これを使用すると、Stable Diffusionに特化した設定やオプションを簡単に利用できます。\n",
        "- 主な機能:\n",
        "  - 特定のプロンプトに基づいて画像を生成します。\n",
        "  - モデルの設定がStable Diffusion向けに最適化されているため、使いやすさが向上します。"
      ],
      "metadata": {
        "id": "gfQwhHhPwBka"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# HuggingFaceアカウントと紐付ける\n",
        "from huggingface_hub import notebook_login\n",
        "notebook_login() # Hugging Faceにログイン（Access TokensのValueを入力）\n",
        "\n",
        "# # terminal等の他環境の場合はhuggingface-cli loginを実行(*にアクセストークンをコピペする)\n",
        "# !huggingface-cli login --token ***************"
      ],
      "metadata": {
        "id": "ngGZlzSDc7Xy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#<font color=fbb03b>使用するモデルについて\n",
        "- 使用するモデルの指定\n",
        "- モデルを学習済みのものから読み込み"
      ],
      "metadata": {
        "id": "CecFddHowWYO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.DiffusionPipelineを使用する場合"
      ],
      "metadata": {
        "id": "oYcvHMTgS6XP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_id = 'gsdf/Counterfeit-V2.5'  # 使用するモデルIDを指定\n",
        "pipe = DiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16).to(\"cuda\")  # モデルを事前学習済みのものから読み込む"
      ],
      "metadata": {
        "id": "ITerjVgLz7Fw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Hugging Faceモデルハブで公開されているモデルのIDを指定します。ここでは例として'gsdf/Counterfeit-V2.5'という名前のモデルを使用しています。\n",
        " - モデルID: Hugging Faceで管理されている拡散モデルの識別子。\n",
        "\n",
        "- from_pretrainedメソッドを使って、指定したmodel_idの事前学習済みモデルをHugging Faceのリポジトリから読み込みます。\n",
        " - from_pretrained: 指定したモデルを事前に学習した状態でロードする関数。"
      ],
      "metadata": {
        "id": "T1d76cbXwmod"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.StableDiffusionPipelineを使用する場合"
      ],
      "metadata": {
        "id": "kHe6bTpXTNLl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Stable Diffusionを使用して高品質なリアルな画像を生成するための一般的なモデル\n",
        "model_id = \"stabilityai/stable-diffusion-2\" # 使用するモデルを入力\n",
        "pipe = StableDiffusionPipeline.from_pretrained(model_id, use_auth_token=True, torch_dtype=torch.float16).to(\"cuda\")"
      ],
      "metadata": {
        "id": "h_zX0VU-yn-h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### StableDiffusionPipeline使用の補足説明\n",
        "以下の3パターンのコードは、`StableDiffusionPipeline` を初期化する方法ですが、それぞれ異なるオプションが指定されています。これらの違いを説明します。\n",
        "\n",
        "### 1. `pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16).to(\"cuda\")`\n",
        "- **torch_dtype=torch.float16**: このオプションは、モデルの重みを16ビット浮動小数点（`float16`）形式で読み込むことを指定します。これにより、メモリ使用量を削減し、GPU上での計算を高速化することが可能です。\n",
        "- **.to(\"cuda\")**: モデルをGPUに転送するためのメソッドです。これにより、GPUを利用してより高速な計算を行うことができます。特に、画像生成タスクでは処理速度が向上します。\n",
        "\n",
        "### 2. `pipe = StableDiffusionPipeline.from_pretrained(model_id)`\n",
        "- **デフォルト設定**: こちらのコードは、特にオプションを指定せずにモデルを読み込む方法です。デフォルトでは、モデルは32ビット浮動小数点（`float32`）形式でロードされ、CPUまたはGPUに明示的に転送されることはありません。\n",
        "- **GPUに自動転送されない**: 明示的に`.to(\"cuda\")`を指定しない限り、モデルはCPU上で実行されます。\n",
        "\n",
        "### 3. `pipe = StableDiffusionPipeline.from_pretrained(model_id, use_auth_token=True)`\n",
        "- **use_auth_token=True**: このオプションは、Hugging Face Hubに対する認証トークンを使用して、プライベートモデルやアクセス制限のあるモデルを取得することを指定します。このトークンが必要な場合に、正しい認証が行われるようにします。\n",
        "- **その他の設定はデフォルト**: モデルはデフォルトの32ビット浮動小数点形式で読み込まれ、CPU上で実行されます（GPUに移動する場合は `.to(\"cuda\")` が必要）。\n",
        "\n",
        "### まとめ\n",
        "- **メモリと速度**: `torch_dtype=torch.float16` は、メモリ効率と速度向上を図るためのオプションです。\n",
        "- **GPUの使用**: `.to(\"cuda\")` を使用すると、計算速度が向上しますが、デフォルトの設定ではCPUで実行されます。\n",
        "- **認証**: `use_auth_token=True` を指定することで、プライベートモデルへのアクセスが可能になります。\n",
        "\n",
        "これにより、各パターンは異なる使用目的や実行環境に応じて最適化されています。"
      ],
      "metadata": {
        "id": "HAari6m8bI7b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3(番外編).LoRAを導入してDiffusionPipelineを使用する場合\n",
        "\n",
        "LoRA (Low-Rank Adaptation) を使用することで、少ない計算リソースでStable Diffusionモデルをカスタマイズできます。ここでは、Dreamboothを用いたLoRAの学習方法と、学習済みLoRAの適用方法を解説します。\n",
        "\n",
        "Dreambooth以外にも、`train_network.py`スクリプトを用いたLoRA学習方法（LoRA訓練スクリプト）もありますが、ここでは割愛します。より高度なカスタマイズをしたい場合は、そちらも検討してみてください。\n",
        "\n",
        "- **Dreambooth**:\n",
        "少数の画像（3-5枚程度）から新しい概念を学習させるための手法です。特定のキャラクターやスタイルを学習させたい場合に有効です\n",
        "\n",
        "- **LoRA訓練スクリプト**:\n",
        "Dreamboothよりも柔軟性が高く、大量のデータで学習させることができます。特定のスタイルや構図を学習させたい場合に適しています。kohya-ssのtrain_network.pyがよく利用されます。"
      ],
      "metadata": {
        "id": "Zes1Ydy97gSd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1. Dreamboothを活用したLoRAの学習\n",
        "\n",
        "\n",
        "1. **環境設定:** Google Colabで、必要なライブラリをインストールします。\n",
        "\n",
        "- LoRA関連ライブラリのインストール: peft, accelerate, bitsandbytes をインストール。\n",
        "\n",
        "```python\n",
        "# 必要なライブラリをインストール\n",
        "from google.colab import output\n",
        "!pip install diffusers transformers peft accelerate bitsandbytes\n",
        "output.clear()\n",
        "\n",
        "# 必要なライブラリをインポート\n",
        "import os\n",
        "import torch\n",
        "from diffusers import DiffusionPipeline\n",
        "from peft import PeftModel, LoraConfig, get_peft_model\n",
        "import matplotlib.pyplot as plt\n",
        "```\n",
        "\n",
        "2. **訓練データの準備:** 学習させたい画像を `/content/drive/MyDrive/your_image_folder` などの、Google Drive 上のフォルダにアップロードします。  `your_image_folder` は任意のフォルダ名に変更してください。\n",
        "\n",
        "```python\n",
        "# 事前に必要なディレクトリの作成\n",
        "os.makedirs(\"/content/drive/My Drive/Colab Notebooks/【画像生成】Stable Dffusion/your_image_folder\", exist_ok=True)\n",
        "os.makedirs(\"/content/drive/My Drive/Colab Notebooks/【画像生成】Stable Dffusion/your_output_folder\", exist_ok=True)\n",
        "```\n",
        "\n",
        "3. **訓練スクリプトの実行:**  Hugging Face Diffusersのexamplesにある`dreambooth`スクリプトを使用します。\n",
        "\n",
        "```python\n",
        "!accelerate launch examples/dreambooth/train_dreambooth.py \\\n",
        "  --pretrained_model_name_or_path=\"gsdf/Counterfeit-V2.5\" \\ # ベースとなるモデルを指定\n",
        "  --instance_data_dir=\"/content/drive/My Drive/Colab Notebooks/【画像生成】Stable Dffusion/your_image_folder\" \\ # 学習画像のフォルダパスを指定(Google Driveを使用する場合)\n",
        "  --instance_prompt=\"a photo of a sks person\" \\ # 学習画像のプロンプトを指定. a photo of a [your_keyword] personのように、[your_keyword]を固有のキーワードに置き換える.\n",
        "  --resolution=512 \\\n",
        "  --train_batch_size=1 \\\n",
        "  --gradient_accumulation_steps=1 \\\n",
        "  --max_train_steps=2000 \\\n",
        "  --learning_rate=5e-6 \\\n",
        "  --scale_lr \\\n",
        "  --lr_scheduler=\"constant\" \\\n",
        "  --lr_warmup_steps=0 \\\n",
        "  --output_dir=\"/content/drive/My Drive/Colab Notebooks/【画像生成】Stable Dffusion/your_output_folder\" \\ # 学習済みモデルの保存先を指定(Google Driveを使用する場合)\n",
        "  --mixed_precision=\"fp16\"\n",
        "\n",
        "  '''\n",
        "  LoRA weightのロード: 訓練が完了したら、--output_dirで指定したフォルダにLoRA weightが保存されます。\n",
        "  '''\n",
        "  ```\n",
        "- `--instance_data_dir`: 学習画像が保存されているフォルダへのパスを指定します。\n",
        "- `--instance_prompt`: 学習対象を特定するためのプロンプトを指定します。  `a photo of a [your_keyword] person` の形式で、`[your_keyword]` を固有の識別子に置き換えます。 例: `a photo of a my_unique_character person`、 `a photo of a my_cat person`。\n",
        "- `--output_dir`: 学習済みモデルの保存先フォルダへのパスを指定します。\n",
        "\n",
        "学習が完了すると、指定した出力フォルダに `pytorch_lora_weights.bin` というファイルが保存されます。これが学習済みの LoRA weight です。"
      ],
      "metadata": {
        "id": "MjLJ4BbDBRnJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2. 学習済みLoRAの設定と適用\n",
        "\n",
        "ここでは、LoRAを適用するための基本的な枠組みを提供しています。\n",
        "\n",
        "LoRAの重みを学習させて読み込むことで、生成画像の精度向上が期待できます。r, lora_alpha, lora_dropout などのLoRAのパラメータや、target_modules を調整することで、さらに精度を高めることができます。\n",
        "\n",
        "```python\n",
        "# LoRA weightを読み込んでモデルに適用\n",
        "# 使用するモデルIDを指定\n",
        "model_id = 'gsdf/Counterfeit-V2.5'\n",
        "\n",
        "# LoRAの設定\n",
        "lora_config = LoraConfig(\n",
        "    r=8,  # LoRAのランク。小さい値はより少ないパラメータを意味します。\n",
        "    lora_alpha=16,  # LoRAのスケーリングファクター。\n",
        "    lora_dropout=0.05,  # LoRAのドロップアウト率。\n",
        "    target_modules=[\"query_key_value\"],  # LoRAを適用するモジュール。\n",
        "    bias=\"none\",\n",
        ")\n",
        "\n",
        "# モデルを事前学習済みのものから読み込む (8-bit で読み込み)\n",
        "pipe = DiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16, variant=\"fp16\") # モデルの読み込み\n",
        "pipe.unet = get_peft_model(pipe.unet, lora_config) # LoRAの適用\n",
        "\n",
        "# 学習済みLoRA weightのロード(事前に学習させたLoRAの重みを想定)\n",
        "pipe.unet.load_state_dict(torch.load(\"/content/drive/My Drive/Colab Notebooks/【画像生成】Stable Dffusion/your_output_folder/pytorch_lora_weights.bin\"), strict=False) # 正確なパスを指定\n",
        "\n",
        "# GPUへの移動\n",
        "pipe.to(\"cuda\")\n",
        "```\n",
        "\n",
        "- LoRAの設定: LoraConfig を使用してLoRAのパラメータを設定。\n",
        "\n",
        "- 8-bit での読み込み: メモリ使用量を削減するため、variant=\"fp16\" を指定して8-bitでモデルを読み込み。\n",
        "\n",
        "- PeftModelの適用: get_peft_model を使用して、LoRAをUNetモデルに適用。\n",
        "\n",
        "- LoRAの重みのロード: pipe.unet.load_state_dict(torch.load(\"/content/drive/MyDrive/your_output_folder/pytorch_lora_weights.bin\"))を追加。事前に学習させたLoRAの重みをロードする必要があります。\"/content/drive/My Drive/Colab Notebooks/【画像生成】Stable Dffusion/your_output_folder/pytorch_lora_weights.bin\" は、学習済みのLoRAの重みのパスに置き換えてください。 この部分が非常に重要で、LoRAの効果はこの重みに依存します。学習済み重みがない場合は、DreamboothやLoRA訓練スクリプトを使用して、自分で学習させる必要があります。\n",
        "\n",
        "※注意: Colabの利用環境により、fp16 (半精度浮動小数点) が動作しない場合もあります。その場合は torch_dtype=torch.float32 に変更してください。"
      ],
      "metadata": {
        "id": "I73vHqm-QyVG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#<font color=fbb03b>画像の生成\n",
        "- 入力するプロンプトを定義\n",
        "- 生成した画像の取得"
      ],
      "metadata": {
        "id": "A1EZAnng2mng"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ポジティブプロンプトとネガティブプロンプトのトークン埋め込みを自動的に連結し、Token長が制限を超える場合でも埋め込みを生成できるようにする関数\n",
        "def token_auto_concat_embeds(pipe, positive, negative):\n",
        "    max_length = pipe.tokenizer.model_max_length\n",
        "    positive_length = pipe.tokenizer(positive, return_tensors=\"pt\").input_ids.shape[-1]\n",
        "    negative_length = pipe.tokenizer(negative, return_tensors=\"pt\").input_ids.shape[-1]\n",
        "\n",
        "    print(f'Token length is model maximum: {max_length}, positive length: {positive_length}, negative length: {negative_length}.')\n",
        "    if max_length < positive_length or max_length < negative_length:\n",
        "        print('Concatenated embedding.')\n",
        "        if positive_length > negative_length:\n",
        "            positive_ids = pipe.tokenizer(positive, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
        "            negative_ids = pipe.tokenizer(negative, truncation=False, padding=\"max_length\", max_length=positive_ids.shape[-1], return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
        "        else:\n",
        "            negative_ids = pipe.tokenizer(negative, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
        "            positive_ids = pipe.tokenizer(positive, truncation=False, padding=\"max_length\", max_length=negative_ids.shape[-1],  return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
        "    else:\n",
        "        positive_ids = pipe.tokenizer(positive, truncation=False, padding=\"max_length\", max_length=max_length,  return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
        "        negative_ids = pipe.tokenizer(negative, truncation=False, padding=\"max_length\", max_length=max_length, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
        "\n",
        "    positive_concat_embeds = []\n",
        "    negative_concat_embeds = []\n",
        "    for i in range(0, positive_ids.shape[-1], max_length):\n",
        "        positive_concat_embeds.append(pipe.text_encoder(positive_ids[:, i: i + max_length])[0])\n",
        "        negative_concat_embeds.append(pipe.text_encoder(negative_ids[:, i: i + max_length])[0])\n",
        "\n",
        "    positive_prompt_embeds = torch.cat(positive_concat_embeds, dim=1)\n",
        "    negative_prompt_embeds = torch.cat(negative_concat_embeds, dim=1)\n",
        "    return positive_prompt_embeds, negative_prompt_embeds"
      ],
      "metadata": {
        "id": "krXwNtjXvKFU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# プロンプトとネガティブプロンプトを定義\n",
        "POSITIVE_PROMPT = \"ここにプロンプトを入力\"\n",
        "\n",
        "NEGATIVE_PROMPT = \"low quality,worst quality,out of focus,ugly,error,JPEG artifacts,low resolution,blurry,bokeh,\" \\\n",
        "                  \"pubic hair,bad anatomy,long_neck,long_body,longbody,deformed,mutated,disfigured,missing arms,\" \\\n",
        "                  \"extra_arms,mutated hands,extra_legs,bad hands,poorly_drawn_hands,malformed_hands,\" \\\n",
        "                  \"missing_limb,floating_limbs,disconnected limbs,extra_fingers,bad fingers,liquid fingers,\" \\\n",
        "                  \"missing fingers,extra digit,fewer digits,ugly face,deformed eyes,\" \\\n",
        "                  \"partial face,partial head,bad face,inaccurate limbs,cropped,wrong perspective,bad proportions,\" \\\n",
        "                  \"oversized limbs,undersized limbs,distorted limbs,twisted body,asymmetrical face,\" \\\n",
        "                  \"misaligned features,weird expressions,uncanny smile,closed eyes when they should be open,\" \\\n",
        "                  \"extra joints,missing joints,disconnected body parts,text,signature,watermark,\" \\\n",
        "                  \"username,artist name,stamp,title,subtitle,date,\" \\\n",
        "                  \"open mouth,half-open eyes,oil painting,sketch,watercolor,2D,flat color,plastic look,\" \\\n",
        "                  \"doll-like,waxy texture,cartoonish,uncanny valley,\" \\\n",
        "                  \"unnatural skin texture,artificial lighting,unrealistic shadows,uncanny skin tone,\" \\\n",
        "                  \"unnatural pose,stiff posture,artificial background,synthetic looking,artificial looking\"\n",
        "\n",
        "# Toekn concatenated embedding\n",
        "positive_embeds, negative_embeds = token_auto_concat_embeds(pipe, POSITIVE_PROMPT, NEGATIVE_PROMPT)"
      ],
      "metadata": {
        "id": "Pk30fxxWs5El"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**解説**: 生成したい画像を説明するテキスト（プロンプト）を入力します。このプロンプトに基づいて拡散モデルが画像を生成します。\n",
        "- プロンプト: モデルが生成するための指示（テキスト）。"
      ],
      "metadata": {
        "id": "ruj2hUIZxTs1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 一枚の画像を生成する方法"
      ],
      "metadata": {
        "id": "n5YAFcVnK4zS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "OL5Fn-bISHf1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # NSFWフィルターを無効化（自己責任）\n",
        "# pipe.safety_checker = None\n",
        "\n",
        "# 一枚の画像を生成\n",
        "img = pipe(prompt_embeds=positive_embeds,\n",
        "           negative_prompt_embeds=negative_embeds,\n",
        "           height=768, width=512, guidance_scale=12,\n",
        "           torch_dtype=torch.float16).images[0] # プロンプトを使って画像を生成し、その画像を取得"
      ],
      "metadata": {
        "id": "NYYcC4oHtXtE",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 画像を取得して表示\n",
        "plt.imshow(img)\n",
        "plt.axis('off')  # 軸を非表示にする\n",
        "plt.show()  # 画像をColab上で表示"
      ],
      "metadata": {
        "id": "UdcSetT0grX3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**解説**: pipeにプロンプトを渡して画像生成を行います。上記の例では生成された1枚の画像を取得しています。\n",
        "- pipe(prompt): プロンプトに基づいて拡散モデルが画像を生成。\n",
        "- .images[0]: 生成された画像リストの中から画像を取り出す。"
      ],
      "metadata": {
        "id": "_BxtD64uyLG5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 生成枚数を指定して複数の画像を生成する方法"
      ],
      "metadata": {
        "id": "jcYP06J4LBGb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "1j2ovSCZKFlT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # NSFWフィルターを無効化（自己責任）\n",
        "# pipe.safety_checker = None\n",
        "\n",
        "# 画像の枚数を指定\n",
        "num_images = 5 # 生成する画像の数を指定\n",
        "images = pipe(prompt_embeds=positive_embeds,\n",
        "              negative_prompt_embeds=negative_embeds,\n",
        "              height=768, width=512, guidance_scale=12,\n",
        "              num_images_per_prompt=num_images,\n",
        "              torch_dtype=torch.float16).images[:] # プロンプトを使って画像を生成し、取得する画像を指定"
      ],
      "metadata": {
        "id": "brVwgFDj78JH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 画像を取得して表示\n",
        "for i, img in enumerate(images[:5]):\n",
        "    plt.imshow(img)\n",
        "    plt.axis('off')  # 軸を非表示にする\n",
        "    plt.show()  # 画像をColab上で表示"
      ],
      "metadata": {
        "id": "PbbJfy32gWUC",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**解説**: pipeにプロンプトを渡して画像生成を行います。生成された複数の画像のうち、指定された画像を取得しています。\n",
        "- pipe(prompt, num_images_per_prompt=num_images): プロンプトに基づいて拡散モデルが画像を生成。\n",
        "  - num_images_per_promptを使って、必要な画像の枚数を指定します。たとえば、num_images_per_prompt = 10と設定すると、10枚の画像が生成されます。\n",
        "- .images[:5]: 生成された画像リストの中から1から5枚目（listの0~4番目）の画像を取り出す。\n",
        "  - Pythonのリストでは、list[start:end]の形式でスライスが可能です。この場合、最初の5枚を取得するにはimages[:5]と書きます。"
      ],
      "metadata": {
        "id": "44Rqm44RMhCa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#<font color=fbb03b>生成した画像の保存"
      ],
      "metadata": {
        "id": "0TrJtqOm2dk6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 一枚の画像を生成する方法で取得した画像の保存"
      ],
      "metadata": {
        "id": "cHz-Snc79evY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# img.save(\"ファイル名.jpg\") # 生成された画像を指定したファイル名で保存\n",
        "img.save(\"/content/drive/MyDrive//Colab Notebooks/【画像生成】Stable Dffusion/Image_storage_folder/ファイル名.jpg\") # ファイル名を入力してその画像ファイルをGoogle Driveに保存"
      ],
      "metadata": {
        "id": "bSFSpcwJtfH3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**解説**: 生成した画像を「ファイル名（ここにファイル名を入力する）.jpg」として保存します。ここで指定したファイル名で保存先が決まります。\n",
        "- img.save: 画像を指定された名前のファイルとして保存するメソッド。"
      ],
      "metadata": {
        "id": "vgwABf5vyfQs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 画像の枚数を指定する方法で取得した画像の保存"
      ],
      "metadata": {
        "id": "rAVmfB-79mli"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example:1～4枚目(listの0,1,2,3)の画像を取得\n",
        "my_images = images[:4]\n",
        "\n",
        "# それぞれの画像を保存\n",
        "for i, img in enumerate(my_images):\n",
        "    img.save(f\"/content/drive/MyDrive//Colab Notebooks/【画像生成】Stable Dffusion/Image_storage_folder/image_{i+1}.jpg\")  # \"image_1.jpg\", \"image_2.jpg\", \"image_3.jpg\", \"image_4.jpg\" というファイル名で保存"
      ],
      "metadata": {
        "id": "Zk4_S0LI9KS3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**解説**:\n",
        "\n",
        "このコードでは、生成された画像を my_images にスライスで格納しています。それぞれの画像は image_1.jpg、image_2.jpg、image_3.jpg、image4.jpg という名前で保存されます。"
      ],
      "metadata": {
        "id": "phHWvepyJzqr"
      }
    }
  ]
}